{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PyTorch_high_level_apis.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"pKVeLItnS5uV"},"source":["Copyright (C) 2020 Software Platform Lab, Seoul National University\n","\n","Licensed under the Apache License, Version 2.0 (the \"License\"); \n","\n","you may not use this file except in compliance with the License. \n","\n","You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 \n","\n","Unless required by applicable law or agreed to in writing, software \n","\n","distributed under the License is distributed on an \"AS IS\" BASIS, \n","\n","\n","WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n","\n","\n","See the License for the specific language governing permissions and\n","\n","\n","limitations under the License."]},{"cell_type":"markdown","metadata":{"id":"2LQHVwI1g_PF"},"source":["### Preparing MNIST Dataset\n","\n","Pytorch is providing a MNIST Dataset class, so we can simply import it from `torchvision.datasets` without handcrafting it. Before instantiating Dataset objects, let's first define transforms for MNIST dataset. `torchvision.transforms` module contains multiple predefined transforms. Here, we apply `ToTensor()` that converts the data into the PyTorch Tensor type and `Normalize(mean, std)` that normalizes the sample data to have given mean and standard deviation. "]},{"cell_type":"code","metadata":{"id":"clLlHNHGz36J","executionInfo":{"status":"ok","timestamp":1605247589146,"user_tz":-540,"elapsed":788,"user":{"displayName":"강민수","photoUrl":"","userId":"02601739229520810364"}}},"source":["import torchvision.transforms as transforms\n","\n","# Normalize data with mean=0.5, std=1.0\n","mnist_transform = transforms.Compose([\n","    transforms.ToTensor(), # dataset으로 받은 값이 tensor가 아닐수도 있기 때문\n","    transforms.Normalize((0.5,), (1.0,))\n","])"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cOUK49KAz36N"},"source":["We use `torchvision.datasets.MNIST` API to instantiate MNIST Dataset objects. Having `download=True` flag, MNIST dataset will be automatically downloaded at `download_root` unless it already exists."]},{"cell_type":"code","metadata":{"id":"LydTwAzMhHw0","executionInfo":{"status":"ok","timestamp":1605247589147,"user_tz":-540,"elapsed":780,"user":{"displayName":"강민수","photoUrl":"","userId":"02601739229520810364"}}},"source":["from torchvision.datasets import MNIST\n","\n","# download path\n","download_root = './MNIST_DATASET'\n","\n","train_dataset = MNIST(download_root, transform=mnist_transform, train=True, download=True)\n","test_dataset = MNIST(download_root, transform=mnist_transform, train=False, download=True)"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jE6MBKTYz36R"},"source":["Finally, we instantiate DataLoader that can shuffles and batches the MNIST Dataset. "]},{"cell_type":"code","metadata":{"id":"g_E6ELImz36S","executionInfo":{"status":"ok","timestamp":1605247589148,"user_tz":-540,"elapsed":774,"user":{"displayName":"강민수","photoUrl":"","userId":"02601739229520810364"}}},"source":["from torch.utils.data import DataLoader\n","\n","BATCH_SIZE = 64 # batch size는 GPU에서 처리하기 용이하도록 2의 거듭제곱으로 한다.\n","\n","train_loader = DataLoader(dataset=train_dataset, \n","                         batch_size=BATCH_SIZE,\n","                         shuffle=True)\n","\n","# test_loader = DataLoader(dataset=test_dataset, \n","#                          batch_size=BATCH_SIZE,\n","#                          shuffle=True)\n","test_loader = DataLoader(dataset=test_dataset, \n","                         batch_size=BATCH_SIZE,\n","                         shuffle=False)"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QACEGZ2KAR7m"},"source":["## Custom Neural Network Models \n","\n","PyTorch `torch.nn.Module` allows you to easily make your own custom neural network model. All you need to do is to 1) make a class that inherits `torch.nn.Module` class and 2) define `forward` method. Let's build a simple CNN model using `torch.nn` APIs. \n","\n","* `torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')` \n","* `torch.nn.Linear(in_features, out_features, bias=True)`\n","* `torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)`\n","* `torch.nn.functional.relu(input, inplace=False)`\n","* `torch.nn.functional.softmax(input, dim=None)`\n","\n","You can refer to the following links for more detailed descriptions of `torch.nn` APIs.\n","* https://pytorch.org/docs/stable/nn.html\n","* https://pytorch.org/docs/stable/nn.functional.html"]},{"cell_type":"code","metadata":{"id":"izGiUAELAR7n","executionInfo":{"status":"ok","timestamp":1605247589150,"user_tz":-540,"elapsed":768,"user":{"displayName":"강민수","photoUrl":"","userId":"02601739229520810364"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","class Net(nn.Module):\n","  \n","    def __init__(self):\n","        super(Net, self).__init__()\n","        \n","        self.conv1 = nn.Conv2d(1, 6, 5, 1)\n","        self.pool1 = nn.MaxPool2d(2)\n","\n","        self.conv2 = nn.Conv2d(6, 16, 5, 1)\n","        self.pool2 = nn.MaxPool2d(2)\n","        \n","        self.fc1 = nn.Linear(256, 64)\n","        self.fc2 = nn.Linear(64, 10)\n","    \n","    def forward(self, x):\n","    \n","        # First convolution layer\n","        x = self.conv1(x)\n","        x = F.relu(x)\n","        x = self.pool1(x)\n","        \n","        \n","        # Second convolution layer\n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        x = self.pool2(x)\n","        \n","        # (N, 256)\n","        x = x.view(-1, 256)\n","        \n","        # First fully-connected layer\n","        x = F.relu(self.fc1(x))\n","        \n","        # Second fully-connected layer\n","        x = self.fc2(x)\n","    \n","        return F.softmax(x, dim=1)"],"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9OJowVblz36Z"},"source":["Instantiate the custom neural network model."]},{"cell_type":"code","metadata":{"id":"NQSRf32Zz36Z","executionInfo":{"status":"ok","timestamp":1605247589151,"user_tz":-540,"elapsed":764,"user":{"displayName":"강민수","photoUrl":"","userId":"02601739229520810364"}},"outputId":"930b0136-5c43-45a1-d34b-50b3d336d161","colab":{"base_uri":"https://localhost:8080/"}},"source":["net = Net()\n","print(net)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Net(\n","  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n","  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n","  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (fc1): Linear(in_features=256, out_features=64, bias=True)\n","  (fc2): Linear(in_features=64, out_features=10, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-FenyD3rz36d"},"source":["Define the loss function (`criterion`) and the optimization method (`optimizer`). In this example, cross entropy loss is used as the criterion and SGD is used as the optimizer. By having `net.parameters()` as the input for the optimizer, we are trying to apply SGD to all the trainable parameters that consist of our custom neural network model `net`."]},{"cell_type":"code","metadata":{"id":"N8b4ed4Xz36e","executionInfo":{"status":"ok","timestamp":1605247589152,"user_tz":-540,"elapsed":760,"user":{"displayName":"강민수","photoUrl":"","userId":"02601739229520810364"}}},"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=0.01)"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6nd6Js7Nz36h"},"source":["Finally, connecting altoghether, we can train the model."]},{"cell_type":"code","metadata":{"id":"jiGgTcMrz36i","executionInfo":{"status":"ok","timestamp":1605247811509,"user_tz":-540,"elapsed":223113,"user":{"displayName":"강민수","photoUrl":"","userId":"02601739229520810364"}},"outputId":"2cfd90c6-edcc-4060-c7e7-e1ee95946155","colab":{"base_uri":"https://localhost:8080/"}},"source":["num_epochs = 10 # dataset을 10번 반복\n","\n","for epoch in range(num_epochs):\n","    train_loss = 0.0\n","    # Iteration over the train dataset\n","    for i, data in enumerate(train_loader):\n","        x, label = data\n","        # 1. Initalize gradient values \n","        optimizer.zero_grad() # gradient 값을 0으로 초기화\n","        # 2. Forward propagation\n","        model_output = net(x)\n","        # 3. Calculate loss using the criterion\n","        loss = criterion(model_output, label)\n","        # 4. Back propagation \n","        loss.backward() # grad 값을 구하는 과정\n","        # 5. Weight update\n","        optimizer.step()\n","        \n","        train_loss += loss.item()\n","        \n","    # Print train loss and test accuracy at the end of every epoch   \n","    with torch.no_grad(): # do not forget this\n","        corr_num = 0\n","        total_num = 0\n","        # Iteration overt the test dataset to evaluate the test accuracy\n","        # for _, test in enumerate(test_loader):\n","        for test in test_loader:\n","            test_x, test_label = test\n","            test_output = net(test_x)\n","            pred_label = test_output.argmax(dim=1)\n","            corr = test_label[test_label == pred_label].size(0)\n","            corr_num += corr\n","            total_num += test_label.size(0)\n","    print(\"[Epoch: %d] train loss: %.4f, test acc: %.2f\" \\\n","        % (epoch + 1, train_loss / len(train_loader), corr_num / total_num * 100))\n","    train_loss = 0.0\n","            "],"execution_count":28,"outputs":[{"output_type":"stream","text":["[Epoch: 1] train loss: 2.3024, test acc: 9.80\n","[Epoch: 2] train loss: 2.3018, test acc: 9.80\n","[Epoch: 3] train loss: 2.3009, test acc: 13.22\n","[Epoch: 4] train loss: 2.2993, test acc: 22.03\n","[Epoch: 5] train loss: 2.2959, test acc: 24.09\n","[Epoch: 6] train loss: 2.2741, test acc: 22.25\n","[Epoch: 7] train loss: 2.1209, test acc: 56.11\n","[Epoch: 8] train loss: 1.8803, test acc: 68.40\n","[Epoch: 9] train loss: 1.7712, test acc: 73.23\n","[Epoch: 10] train loss: 1.7251, test acc: 80.57\n"],"name":"stdout"}]}]}