{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF_basic.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwlPAA8ZJUsr"
      },
      "source": [
        "Copyright (C) 2020 Software Platform Lab, Seoul National University\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); \n",
        "\n",
        "you may not use this file except in compliance with the License. \n",
        "\n",
        "You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 \n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software \n",
        "\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS, \n",
        "\n",
        "\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n",
        "\n",
        "\n",
        "See the License for the specific language governing permissions and\n",
        "\n",
        "\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ki_RHIwPJvyn"
      },
      "source": [
        "# **1. TensorFlow Operations**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5N1npMVQqJz"
      },
      "source": [
        "## Constant Tensor\n",
        "\n",
        "Let's create a constant tensor in TensorFlow.\n",
        "\n",
        "**```tf.constant(\n",
        "    value, dtype=None, shape=None, name='Const'\n",
        ")```**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFZ3bfVsQz_p",
        "outputId": "b4a66052-a98b-4fbf-8263-2660cef4aff0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# constant of 1d tensor, or a vector\n",
        "a = tf.constant([2,2], name = 'vector')\n",
        "\n",
        "# constant of 2x2 tensor, or a matrix\n",
        "b = tf.constant([[0,2], [1,3]], name = 'matrix')\n",
        "\n",
        "print(a.numpy())\n",
        "print(b.numpy())"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2 2]\n",
            "[[0 2]\n",
            " [1 3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrE4WkZNUn9o"
      },
      "source": [
        "## Mathematical Operations\n",
        "\n",
        "The following example shows a matrix division operation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drceRvn4VGec",
        "outputId": "09865248-c04a-473a-d041-5b1a7adac429",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create constant tensors a and b\n",
        "a = tf.constant([2,4], name = 'a', dtype = tf.float32)\n",
        "b = tf.constant([[0,1], [2,3]], name = 'b', dtype = tf.float32)\n",
        "  \n",
        "# Execute division operation using b and a\n",
        "div = tf.divide(b, a)\n",
        "# div = b / a # same\n",
        "\n",
        "# def __div__(self, val) # magic method\n",
        "\n",
        "print('\\nPrint div')\n",
        "print(div.numpy())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Print div\n",
            "[[0.   0.25]\n",
            " [1.   0.75]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnNhG69dgWsA"
      },
      "source": [
        "## Quiz 1\n",
        "**Create two constants with shape=[2,2] and perform matrix multiplication. Print the result using c. (HINT: use `tf.matmul`)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZyw665-gWsE",
        "outputId": "ec279d64-8ac1-4aff-9886-323bfaf34a20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def matmul(x: list, y: list):\n",
        "  ############# Write here. ################\n",
        "  x_tf = tf.constant(x)\n",
        "  y_tf = tf.constant(y)\n",
        "  return tf.matmul(x,y)\n",
        "  ##########################################\n",
        "x = [[1, 2], [3, 4]]\n",
        "y = [[5, 6], [7, 8]]\n",
        "z = matmul(x, y)\n",
        "print(z.numpy())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[19 22]\n",
            " [43 50]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4UKKwwsZa4I"
      },
      "source": [
        "## Variables\n",
        "\n",
        "Shared, mutable states (e.g., model parameters)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TubsFPeZ0Rz"
      },
      "source": [
        "### Creating Variable\n",
        "\n",
        "To declare a variable, you create an instance of the class `tf.Variable`.\n",
        "\n",
        "#### Usage of TF Variable\n",
        "\n",
        "\n",
        "```\n",
        "x = tf.Variable(...)\n",
        "x.read_value()      # read value\n",
        "x.assign(...)       # x = ...\n",
        "x = 1.0 등으로 값을 선언할 경우 variable의 값을 변화시키는 것이 아니라 단순히 x에 새로운 값을 할당하기 때문에 사용 X\n",
        "x.assign_add(...)   # x += ...\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcriAnGbaCKk"
      },
      "source": [
        "One way to create a variable is: \n",
        "\n",
        "**```tf.Variable(< initial-value >, name = < optional-name >)```**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsaTk0M3hr0p"
      },
      "source": [
        "This example creates three variables using `tf.Variable`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32t-sxgsaFXh"
      },
      "source": [
        "# Create scalar variable\n",
        "s = tf.Variable(2, name = 'scalar')\n",
        "# Create matrix variable\n",
        "m = tf.Variable([[0,1], [2,3]], name = 'matrix')\n",
        "# Create zero matrix using tf.zeros\n",
        "W = tf.Variable(tf.zeros([784,10]))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O91x5GeZzW2N",
        "outputId": "3c7d0b3e-8cda-4840-da3c-e6771530294b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Print values of Variable s, m and W\n",
        "print('s:')\n",
        "print(s.read_value().numpy())\n",
        "print('\\nm:')\n",
        "print(m.read_value().numpy())\n",
        "print('\\nW:')\n",
        "print(W.read_value().numpy())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "s:\n",
            "2\n",
            "\n",
            "m:\n",
            "[[0 1]\n",
            " [2 3]]\n",
            "\n",
            "W:\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "239nTB8ScEMD"
      },
      "source": [
        "### Changing values of variables\n",
        "\n",
        "To change the value of a variable, we need to assign a new value to the variable.\n",
        "You can see variable `v` changes after `assign` operations are executed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ezu6kFnScVd8",
        "outputId": "22f46152-f44e-40b9-b04a-9af3379a3407",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# v is a 2 x 3 variable of random values\n",
        "initializer = tf.random_normal_initializer(mean=1., stddev=2.)\n",
        "v = tf.Variable(initializer(shape=[2, 3]))\n",
        "\n",
        "# c is a 2 x 3 constant with 1.0\n",
        "c = tf.constant(1.0, shape=(2,3))\n",
        "# assign_1 = v.assign(c)\n",
        "# assign_2 = v.assign([[1., 2., 3.], [4., 5., 6.]])\n",
        "\n",
        "# Get value\n",
        "print('v:')\n",
        "print(v.read_value().numpy())\n",
        "\n",
        "# Assign new value to the variable\n",
        "v.assign(c)\n",
        "\n",
        "# Get value again\n",
        "print('v:')\n",
        "print(v.read_value().numpy())\n",
        "\n",
        "# Assign new value to the variable\n",
        "v.assign([[1., 2., 3.], [4., 5., 6.]])\n",
        "\n",
        "# Get value again\n",
        "print('v:')\n",
        "print(v.read_value().numpy())\n",
        "\n",
        "w = v\n",
        "v = 0.1 \n",
        "print(\"v \\n\", v)\n",
        "print(\"w \\n\", w.read_value().numpy()) # tensor에는 전혀 영향을 미치지 못한다."
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "v:\n",
            "[[ 2.6910524  -0.88866615  3.0349424 ]\n",
            " [ 2.3670192   0.03184408  0.23808438]]\n",
            "v:\n",
            "[[1. 1. 1.]\n",
            " [1. 1. 1.]]\n",
            "v:\n",
            "[[1. 2. 3.]\n",
            " [4. 5. 6.]]\n",
            "v \n",
            " 0.1\n",
            "w \n",
            " [[1. 2. 3.]\n",
            " [4. 5. 6.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gs3yizqwmxHs"
      },
      "source": [
        "## Quiz 2\n",
        "Define a variable (name : \"term\") with shape = [] and dtype = `tf.float64`. Initialize the variable as `2` first.\n",
        "Define another variable (name : \"sum\") with shape = [] and dtype = `tf.float64`. Initialize the variable as zeros.\n",
        "\n",
        "By using these two variables, compute the following:\n",
        "$sum = 1/term_1 + 1/term_2 + ... + 1/term_{10}$\n",
        "where\n",
        "$term_i = term_{i-1} * (term_{i-1} - 1) + 1$\n",
        "and $term_1 = 2$.\n",
        "(This recurrence relation is known as Sylvester's sequence.)\n",
        "\n",
        "Hint: Repeat updating the variables \"sum\" and \"term\" 10 times.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhqP7g40o0z2",
        "outputId": "92de87c1-8aaa-4751-bfdc-180210265220",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "############# Write here. ################\n",
        "term = tf.Variable(2.0, name = 'term', shape = [], dtype = tf.float64) # shape = [] : scalar 값을 의미\n",
        "sum = tf.Variable(0.0, name='sum', shape = [], dtype = tf.float64)\n",
        "for _ in range(10):\n",
        "  sum.assign_add(1 / term)\n",
        "  term.assign(term*(term-1)+1)\n",
        "##########################################\n",
        "\n",
        "print('sum:', sum.read_value().numpy())\n",
        "print(sum)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sum: 0.9999999999999999\n",
            "<tf.Variable 'sum:0' shape=() dtype=float64, numpy=0.9999999999999999>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbiAmxry-n75"
      },
      "source": [
        "# **2. Dataset API**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mHYbZmS06zb"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The `tf.data` API is the most advanced API for writing TensorFlow input pipelines.\n",
        "\n",
        "It allows you to build complex pipelines by composing simple building blocks. \n",
        "\n",
        "`tf.data.Dataset` is an abstraction representing a sequence of elements (each element represents one or more `tf.Tensor`s)\n",
        "\n",
        "Users can create new Datasets from existing `tf.Tensor`s by using static methods like `Dataset.from_tensor_slices()`. \n",
        "\n",
        "For example, you can create a Dataset of string Tensors that represents input file names. \n",
        "\n",
        "Transformation of exisiting Datasets is another way of creating new dataset. \n",
        "\n",
        "TensorFlow provides frequently-used Dataset transformations such as `Dataset.batch` or `Dataset.shuffle` (please refer to https://www.tensorflow.org/api_docs/python/tf/data/Dataset). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2N-qVWB4VNW"
      },
      "source": [
        "### `tf.data.Dataset.from_tensor_slices()`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knXKv93E4drK"
      },
      "source": [
        "Creates a Dataset whose elements are slices of the given *python array* or *numpy array* or *tensors*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lVbbwCfzGbx",
        "outputId": "c8989b5e-25f9-4ac0-d377-2ab6c874af0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "arr = np.arange(10) # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "\n",
        "# Create a dataset from a numpy array\n",
        "dataset = tf.data.Dataset.from_tensor_slices(arr)\n",
        "\n",
        "# Iterate through the dataset\n",
        "for element in dataset:\n",
        "  # Print multiplied value for each element in the dataset\n",
        "  print((element * 2).numpy())\n",
        "\n",
        "# You can also use an iterator like this:\n",
        "iterator = iter(dataset) # python 내장함수 이용\n",
        "print(iterator)\n",
        "for _ in range(10):\n",
        "  print(next(iterator).numpy())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "2\n",
            "4\n",
            "6\n",
            "8\n",
            "10\n",
            "12\n",
            "14\n",
            "16\n",
            "18\n",
            "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7fd95a9d47f0>\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hV-LVSuslOLE"
      },
      "source": [
        "## Create a dataset from files using the Dataset API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCt7x4ByogNm"
      },
      "source": [
        "### Create dummy binary files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTFhI2kBomkO"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def create_bin_file(file_name, value):\n",
        "  with open(file_name, 'wb') as f:\n",
        "    f.write(np.arange(value, value+4, dtype=np.int32))\n",
        "    \n",
        "bin_filenames = []\n",
        "for i in range(3):\n",
        "  file_name = 'binary_file_%d'% i\n",
        "  create_bin_file(file_name, i)\n",
        "  bin_filenames.append(file_name)\n",
        "\n",
        "# first file:\n",
        "# 0 1 2 3\n",
        "\n",
        "# second file:\n",
        "# 1 2 3 4\n",
        "\n",
        "# third file:\n",
        "# 2 3 4 5"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rBvsoONqBSl"
      },
      "source": [
        "### FixedLengthRecordDataset : each fixed-length slice of bytes is a dataset element.\n",
        "\n",
        "In this example, each data instance is a 8-byte integer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48JvZOGtqEwY",
        "outputId": "3336681a-fcbc-4d92-f733-989e406da427",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# create a Dataset that contains slices (size: 8 bytes) of the files\n",
        "dataset = tf.data.FixedLengthRecordDataset(bin_filenames, 8) # 파일을 일정한 size로 끊어서 읽는다. 32bit 정수 2개씩 읽는다.\n",
        "# or equivalently,\n",
        "# ds = tf.data.Dataset.from_tensor_slices(bin_filenames)\n",
        "# ds = ds.apply(lambda filename: tf.data.FixedLengthRecordDataset(filename, 8))\n",
        "\n",
        "# Iterate through the dataset\n",
        "for i, element in enumerate(dataset):\n",
        "  # convert 8 bytes into int32 => two int32 value per each element\n",
        "  print('step %d, data: %s' % (i, tf.io.decode_raw(element, 'int32').numpy()))\n",
        "  # print('step %d, data: %s' % (i, element.numpy()))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, data: [0 1]\n",
            "step 1, data: [2 3]\n",
            "step 2, data: [1 2]\n",
            "step 3, data: [3 4]\n",
            "step 4, data: [2 3]\n",
            "step 5, data: [4 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5FwnpyyneZQ"
      },
      "source": [
        "### Create dummy text files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdSZO50jlSbB"
      },
      "source": [
        "def create_text_file(file_name, index):\n",
        "  with open(file_name, 'w') as f:\n",
        "    f.write('Hello_%d\\n' % index)\n",
        "    f.write('TensorFlow_%d\\n' % index)\n",
        "\n",
        "text_filenames = []\n",
        "for i in range(3):\n",
        "  file_name = 'text_file_%d'% i\n",
        "  create_text_file(file_name, i)\n",
        "  text_filenames.append(file_name)\n",
        "\n",
        "# first file:\n",
        "# Hello_0\n",
        "# TensorFlow_0\n",
        "\n",
        "# second file:\n",
        "# Hello_1\n",
        "# TensorFlow_1\n",
        "\n",
        "# third file:\n",
        "# Hello_2\n",
        "# TensorFlow_2"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv2Vc8gt9WbO"
      },
      "source": [
        "### TextLineDataset : each text line is a dataset element.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw5gAwbcCNPm"
      },
      "source": [
        "def iterate_and_print(iterator, count=6):\n",
        "  for i in range(count):\n",
        "    print('step %d, data: %s' % (i, next(iterator).numpy()))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYY0TdipmvoU",
        "outputId": "021fd3ab-463b-48e1-e4cb-2e2b188e0e79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# create a Dataset that contains each line of the text files\n",
        "ds = tf.data.TextLineDataset(text_filenames)\n",
        "# or equivalently,\n",
        "# ds = tf.data.Dataset.from_tensor_slices(text_filenames)\n",
        "# ds = ds.apply(lambda filename: tf.data.TextLineDataset(filename))\n",
        "\n",
        "# Create iterator for the dataset\n",
        "iterator = iter(ds)\n",
        "\n",
        "iterate_and_print(iterator) # 출력된 결과에 b가 붙는 것은 byte array를 의미한다."
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, data: b'Hello_0'\n",
            "step 1, data: b'TensorFlow_0'\n",
            "step 2, data: b'Hello_1'\n",
            "step 3, data: b'TensorFlow_1'\n",
            "step 4, data: b'Hello_2'\n",
            "step 5, data: b'TensorFlow_2'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8eqeLjir-ar"
      },
      "source": [
        "## Transform dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GinK40Kp5jQA"
      },
      "source": [
        "**ds.shuffle(buffer_size)**\n",
        "\n",
        "shuffle: shuffle data instances randomly. buffer size represents the number of data instances to be sampled.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzImNht48XKW"
      },
      "source": [
        "`ds.shuffle` with N > 1 can pick data instances randomly from the buffer containing N instances. The code snippet below shows that we always do not get the 5th or 6th element of the dataset (Hello_2 or TensorFlow_2) at step 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3oaSOHVsGLc",
        "outputId": "b4aff1cd-4c76-4487-b2bb-76fcb0af178f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# dataset : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "# buffer_size = 4 buffer size가 어느정도 적당히 있으면 shuffle의 효과를 가진다.\n",
        "# [0, 1, 2, 3] -> 4개 중 하나를 random하게 골라서 return. ex) 2\n",
        "# [0, 1, 3, 5] -> 1\n",
        "# [0, 3, 5, 6] -> 7 ...\n",
        "# 완벽한 shuffle은 아니지만 disk에서 memory로 dataset을 읽어올 때 앞쪽부터 한번에 읽어오는게 random하게 읽어오는 것보다 빠르기 때문에 사용\n",
        "\n",
        "# Load the text file created previously\n",
        "ds = tf.data.TextLineDataset(text_filenames) \n",
        "# shuffle the dataset using buffer size 4\n",
        "ds = ds.shuffle(4)\n",
        "\n",
        "iterator = iter(ds)\n",
        "iterate_and_print(iterator)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, data: b'TensorFlow_1'\n",
            "step 1, data: b'TensorFlow_0'\n",
            "step 2, data: b'TensorFlow_2'\n",
            "step 3, data: b'Hello_1'\n",
            "step 4, data: b'Hello_0'\n",
            "step 5, data: b'Hello_2'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP-MH37I8s8z"
      },
      "source": [
        "`ds.shuffle` with N == 1 has no shuffling effect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8Q9W2pG6nxF",
        "outputId": "22fbe3f4-fdf2-4712-bd58-140a9e3f12f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Load the text file created previously\n",
        "ds = tf.data.TextLineDataset(text_filenames)\n",
        "# shuffle the dataset using buffer size 1\n",
        "ds = ds.shuffle(1) # shuffle이 전혀 되지 않는다.\n",
        "\n",
        "iterator = iter(ds)\n",
        "iterate_and_print(iterator)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, data: b'Hello_0'\n",
            "step 1, data: b'TensorFlow_0'\n",
            "step 2, data: b'Hello_1'\n",
            "step 3, data: b'TensorFlow_1'\n",
            "step 4, data: b'Hello_2'\n",
            "step 5, data: b'TensorFlow_2'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i-rJwnE6Vdj"
      },
      "source": [
        "**ds.repeat(count)**\n",
        "\n",
        "Repeat the data instances count times. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM9xpfjr72Qo"
      },
      "source": [
        "An error is raised when an iterator calls a next element after reading all the data from the dataset. `ds.repeat(count)` repeats the dataset `ds` so each original value is seen `count` times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUzyWsUt6WQ_",
        "outputId": "8f34767c-f26c-4e5c-9aaa-ca1c1cf43874",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ds = tf.data.TextLineDataset(text_filenames)\n",
        "\n",
        "iterator = iter(ds)\n",
        "iterate_and_print(iterator, count=6)\n",
        "# iterate_and_print(iterator, count=7) # error. 길이가 6인 dataset을 7번 iterate했기 때문"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, data: b'Hello_0'\n",
            "step 1, data: b'TensorFlow_0'\n",
            "step 2, data: b'Hello_1'\n",
            "step 3, data: b'TensorFlow_1'\n",
            "step 4, data: b'Hello_2'\n",
            "step 5, data: b'TensorFlow_2'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_CwPo5I76G9"
      },
      "source": [
        "`ds.repeat(count)` repeats iterating the dataset `count` times. If we do not pass the `count` argument, the dataset repeats forever."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzqJY23X8Hg0",
        "outputId": "d252f9a0-d172-45d7-94ab-d5409c56c19d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ds = tf.data.TextLineDataset(text_filenames)\n",
        "\n",
        "# repeat twice\n",
        "ds = ds.repeat(2)\n",
        "\n",
        "iterator = iter(ds)\n",
        "iterate_and_print(iterator, count=12)\n",
        "# iterate_and_print(iterator, count=13) # error. 6의 2배인 12인데 13번 iterate했기 때문"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, data: b'Hello_0'\n",
            "step 1, data: b'TensorFlow_0'\n",
            "step 2, data: b'Hello_1'\n",
            "step 3, data: b'TensorFlow_1'\n",
            "step 4, data: b'Hello_2'\n",
            "step 5, data: b'TensorFlow_2'\n",
            "step 6, data: b'Hello_0'\n",
            "step 7, data: b'TensorFlow_0'\n",
            "step 8, data: b'Hello_1'\n",
            "step 9, data: b'TensorFlow_1'\n",
            "step 10, data: b'Hello_2'\n",
            "step 11, data: b'TensorFlow_2'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yM9UYyhdamR"
      },
      "source": [
        "Another common pattern is to use try-except clause to detect the end of epoch. Once we finisn an epoch, we re-initialize the iterator to start from the beginning again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyHGXUxadamS",
        "outputId": "566975cc-57ea-49be-a919-ae3bfdf04a8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ds = tf.data.TextLineDataset(text_filenames)\n",
        "\n",
        "iterator = iter(ds)\n",
        "\n",
        "epoch = 0\n",
        "step = 0\n",
        "\n",
        "while True:\n",
        "  # repeat until we detect an error\n",
        "  try:\n",
        "    v = next(iterator).numpy()\n",
        "    print('step %d, data: %s' % (step, v))\n",
        "    step += 1\n",
        "  # iterator raises StopIteration once we finish an epoch\n",
        "  except StopIteration:\n",
        "    print('Finished epoch', epoch)\n",
        "    epoch += 1\n",
        "    # if we are done with 2 epochs, break\n",
        "    if epoch >= 2:\n",
        "      break\n",
        "    # otherwise, re-create an iterator\n",
        "    iterator = iter(ds)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, data: b'Hello_0'\n",
            "step 1, data: b'TensorFlow_0'\n",
            "step 2, data: b'Hello_1'\n",
            "step 3, data: b'TensorFlow_1'\n",
            "step 4, data: b'Hello_2'\n",
            "step 5, data: b'TensorFlow_2'\n",
            "Finished epoch 0\n",
            "step 6, data: b'Hello_0'\n",
            "step 7, data: b'TensorFlow_0'\n",
            "step 8, data: b'Hello_1'\n",
            "step 9, data: b'TensorFlow_1'\n",
            "step 10, data: b'Hello_2'\n",
            "step 11, data: b'TensorFlow_2'\n",
            "Finished epoch 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71CqxItI4_Rk"
      },
      "source": [
        "**ds.batch(batch_size)**\n",
        "\n",
        "Combines elements of this dataset into batches. `batch_size` represents the number of data instances to combine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8stEmpyxvT18",
        "outputId": "d1c95967-8edc-416c-c43a-05bdbf3d9b91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ds = tf.data.TextLineDataset(text_filenames) \n",
        "# batch elements using batch_size 3\n",
        "ds = ds.batch(3)\n",
        "ds = ds.repeat(3)\n",
        "\n",
        "iterator = iter(ds)\n",
        "iterate_and_print(iterator)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, data: [b'Hello_0' b'TensorFlow_0' b'Hello_1']\n",
            "step 1, data: [b'TensorFlow_1' b'Hello_2' b'TensorFlow_2']\n",
            "step 2, data: [b'Hello_0' b'TensorFlow_0' b'Hello_1']\n",
            "step 3, data: [b'TensorFlow_1' b'Hello_2' b'TensorFlow_2']\n",
            "step 4, data: [b'Hello_0' b'TensorFlow_0' b'Hello_1']\n",
            "step 5, data: [b'TensorFlow_1' b'Hello_2' b'TensorFlow_2']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meX0yDAd4KCD"
      },
      "source": [
        "**ds.map(fn)**\n",
        "\n",
        "Apply `fn` to each element of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zITLEwgys7l8",
        "outputId": "a357a086-a2c0-412e-869f-481bd8e9ab3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# split the `data` tensor into 3 pieces and concatenate the pieces by inserting '+' between them\n",
        "def split_join(data):\n",
        "  data = tf.split(data, 3)\n",
        "  return tf.strings.join(data, '+')\n",
        "\n",
        "ds = tf.data.TextLineDataset(text_filenames)\n",
        "ds = ds.batch(3)\n",
        "ds = ds.repeat(3)\n",
        "ds = ds.map(split_join)\n",
        "\n",
        "iterator = iter(ds)\n",
        "iterate_and_print(iterator)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, data: [b'Hello_0+TensorFlow_0+Hello_1']\n",
            "step 1, data: [b'TensorFlow_1+Hello_2+TensorFlow_2']\n",
            "step 2, data: [b'Hello_0+TensorFlow_0+Hello_1']\n",
            "step 3, data: [b'TensorFlow_1+Hello_2+TensorFlow_2']\n",
            "step 4, data: [b'Hello_0+TensorFlow_0+Hello_1']\n",
            "step 5, data: [b'TensorFlow_1+Hello_2+TensorFlow_2']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXqUUZeo5a9E"
      },
      "source": [
        "##  Speed up Dataset processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WplDJMqQ59im"
      },
      "source": [
        "\n",
        "**ds.interleave(map_func, cycle_length)**\n",
        "\n",
        "map_func : map function to apply to each data instance\n",
        "\n",
        "cycle_length : the number of data instances to process concurrently"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elNSpQlF9Kkd"
      },
      "source": [
        "We can use this feature to read and process multiple files concurrently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlIOImd6KHHr",
        "outputId": "33ca8a67-a9cd-43fa-9ee5-653f6da12b11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ds = tf.data.Dataset.from_tensor_slices(text_filenames)\n",
        "# consume the first two files in concurrently, and then the third file \n",
        "ds = ds.interleave(lambda filename: tf.data.TextLineDataset(filename),\n",
        "                    cycle_length=2) # cycle_length = 2 : thread를 2개 만들어서 동시에 실행하겠다. file 0과 file 1을 동시에 받아온다.\n",
        "                    # lambda : inline으로 함수를 정의하고 싶을 때 사용. input : filename, return : tf.data.TextLIneDataset(filename)\n",
        "\n",
        "iterator = iter(ds)\n",
        "iterate_and_print(iterator)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, data: b'Hello_0'\n",
            "step 1, data: b'Hello_1'\n",
            "step 2, data: b'TensorFlow_0'\n",
            "step 3, data: b'TensorFlow_1'\n",
            "step 4, data: b'Hello_2'\n",
            "step 5, data: b'TensorFlow_2'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhp0R08AKxpy"
      },
      "source": [
        "**ds.prefetch(buffer_size)** \n",
        "\n",
        "prefetch elements from a dataset. buffer size represents the maximum buffer size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWM2PH6SLHrb",
        "outputId": "6a17f86f-4aac-4c87-b9d0-613d0bebca21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ds = tf.data.Dataset.from_tensor_slices(text_filenames)\n",
        "ds = ds.interleave(lambda filename: tf.data.TextLineDataset(filename),\n",
        "                    cycle_length=2)\n",
        "ds = ds.prefetch(3) # dataset를 미리 processing하여 가지고 있는다. buffer_size : 몇 개 까지 미리 processing 해 놓을지\n",
        "\n",
        "iterator = iter(ds)\n",
        "iterate_and_print(iterator)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, data: b'Hello_0'\n",
            "step 1, data: b'Hello_1'\n",
            "step 2, data: b'TensorFlow_0'\n",
            "step 3, data: b'TensorFlow_1'\n",
            "step 4, data: b'Hello_2'\n",
            "step 5, data: b'TensorFlow_2'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEs3qEJszYgu"
      },
      "source": [
        "## Quiz 4\n",
        "Create a dataset following the instructions.\n",
        "\n",
        "1. Create a textline dataset using files named `ex_filenames`. \n",
        "2. Shuffle the dataset with buffer size 15.\n",
        "3. Repeat the dataset for 2 epochs.\n",
        "4. Convert each data instance using the `cast` function defined below.\n",
        "5. Make the data instances as a batch (batch size = 3).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_TOoWhx1O2S",
        "outputId": "1c5a2f73-63dc-4b87-9f28-82c193d29e39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import random\n",
        "\n",
        "def create_text_file(index):\n",
        "  with open('ex_file_%d'%index, 'w') as f:\n",
        "    for i in range(3):\n",
        "      f.write('%d.%d\\n' % (index, i))\n",
        "    \n",
        "ex_filenames = []\n",
        "for i in range(5):\n",
        "  create_text_file(i)\n",
        "  ex_filenames.append('ex_file_%d'% i)\n",
        "\n",
        "def cast(data):\n",
        "  data = tf.strings.to_number(data, out_type=tf.float32)\n",
        "  return data\n",
        "\n",
        "\n",
        "############# Write here. ################\n",
        "\n",
        "# Create a Dataset\n",
        "ds = tf.data.TextLineDataset(ex_filenames)\n",
        "# Shuffle\n",
        "ds = ds.shuffle(15)\n",
        "# Repeat\n",
        "ds = ds.repeat(2)\n",
        "# Transformation\n",
        "ds = ds.map(cast)\n",
        "# Create a mini-batch\n",
        "ds = ds.batch(3)\n",
        "\n",
        "##########################################\n",
        "\n",
        "iterator = iter(ds)\n",
        "iterate_and_print(iterator, count=10)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step 0, data: [2.  1.1 1. ]\n",
            "step 1, data: [1.2 0.1 2.1]\n",
            "step 2, data: [4.1 0.  3.1]\n",
            "step 3, data: [2.2 3.  3.2]\n",
            "step 4, data: [0.2 4.2 4. ]\n",
            "step 5, data: [4.1 0.2 4. ]\n",
            "step 6, data: [2.  1.1 0.1]\n",
            "step 7, data: [0.  1.2 3.1]\n",
            "step 8, data: [1.  3.2 2.1]\n",
            "step 9, data: [4.2 3.  2.2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SO0i-ksu8jB9"
      },
      "source": [
        "# **FYI: TensorFlow v1 vs. TensorFlow v2** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDH6BhQ3P7ou"
      },
      "source": [
        "Throughout the tutorial, we used the latest release of TensorFlow. Check out the version:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fn3qNA6uP7BY",
        "outputId": "d40f9e42-1f85-4b51-fb41-a324f62de52a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"TensorFlow version: \", tf.__version__)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version:  2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thHFaK0L8nqp"
      },
      "source": [
        "Previously in TensorFlow v1, we construct a graph using **Graph** which consists of TensorFlow operations (Ops). \n",
        "\n",
        "After defining a Graph, we could run it via **Session**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22S2dhxtqV1m"
      },
      "source": [
        "# graph = tf.Graph()\n",
        "# with graph.as_default():\n",
        "      # Dataset\n",
        "      # Build a model\n",
        "      # Training\n",
        "#     // Here we load dataset, define operations, etc.\n",
        "\n",
        "# with tf.Session(graph=graph) as sess:\n",
        "#     sess.run(...)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXypoQu4Qn53"
      },
      "source": [
        "Switching from TensorFlow v1 to TensorFlow v2, there have been many things changed.\n",
        "To sum up the update, TensorFlow shifted to **eager execution** (imperative execution) by default. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipQBjqopUOWM"
      },
      "source": [
        "Eager execution provides an intuitive interface to structure the code naturally and use Python data structures. It is also easier to debug and test changes by using standard Python debugging tools.\n",
        "Lastly, it has natural Python control flow instead of graph control flow, simplifying the specification of dynamic models. The downside is that eager execution is much slower than symbolic execution, the default TensorFlow v1 mode."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXLES0lRVuVJ"
      },
      "source": [
        "Let's see the difference of the two versions with an example of dynamic control flow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vwwvlb_7V4Fo"
      },
      "source": [
        "## Dynamic control flow\n",
        "A major benefit of eager execution is that all the functionality of the host language is available while your model is executing. So, for example, it is easy to write fizzbuzz game where any number divisible by three is replaced with the word \"fizz\", and any number divisible by five is replaced with the word \"buzz\" (similar to the 3-6-9 game)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhfCip_FV1j3",
        "outputId": "d1130d30-6013-486b-c28c-ed4e3bce28b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Native python code\n",
        "def fizzbuzz(max_num):\n",
        "    counter = 0\n",
        "    for num in range(1, max_num+1):\n",
        "        if int(num % 3) == 0 and int(num % 5) == 0:\n",
        "            print('FizzBuzz')\n",
        "        elif int(num % 3) == 0:\n",
        "            print('Fizz')\n",
        "        elif int(num % 5) == 0:\n",
        "            print('Buzz')\n",
        "        else:\n",
        "            print(num)\n",
        "        counter += 1\n",
        "    \n",
        "fizzbuzz(20)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "Fizz\n",
            "4\n",
            "Buzz\n",
            "Fizz\n",
            "7\n",
            "8\n",
            "Fizz\n",
            "Buzz\n",
            "11\n",
            "Fizz\n",
            "13\n",
            "14\n",
            "FizzBuzz\n",
            "16\n",
            "17\n",
            "Fizz\n",
            "19\n",
            "Buzz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgAzIPm5WJ8R"
      },
      "source": [
        "In eager execution, we need to add minor changes \n",
        "in a few lines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIb2eV37WOPX",
        "outputId": "3399ee5b-13bc-4325-dc68-e0740da451f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def fizzbuzz_eager(max_num):\n",
        "    counter = tf.constant(0) # counter = 0\n",
        "    max_num = tf.convert_to_tensor(max_num) #\n",
        "    for num in range(1, max_num.numpy() + 1): #\n",
        "        num = tf.constant(num) # \n",
        "        if int(num % 3) == 0 and int(num % 5) == 0:\n",
        "            print('FizzBuzz')\n",
        "        elif int(num % 3) == 0:\n",
        "            print('Fizz')\n",
        "        elif int(num % 5) == 0:\n",
        "            print('Buzz')\n",
        "        else:\n",
        "            print(num.numpy()) # print(num)\n",
        "        counter += 1\n",
        "    \n",
        "fizzbuzz_eager(20)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "Fizz\n",
            "4\n",
            "Buzz\n",
            "Fizz\n",
            "7\n",
            "8\n",
            "Fizz\n",
            "Buzz\n",
            "11\n",
            "Fizz\n",
            "13\n",
            "14\n",
            "FizzBuzz\n",
            "16\n",
            "17\n",
            "Fizz\n",
            "19\n",
            "Buzz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QGYXcyyWRzN"
      },
      "source": [
        "Implementing the same thing using graph mode in TensorFlow v1: [Tensorflow FizzBuzz Revisited (Ricky Han blog)](https://rickyhan.com/jekyll/update/2018/02/16/tensorflow-fizzbuzz-revisited.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QpFmaoDWaRU",
        "outputId": "9f611d74-7142-43c2-ca0a-d62584b2faa4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# This code does not run in tensorflow 2.x\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "def fizzbuzz_graph(max_num):\n",
        "    # Define variable and while_loop\n",
        "    graph = tf.Graph()\n",
        "    with graph.as_default():\n",
        "        arr = tf.Variable([str(i) for i in range(1, max_num+1)])\n",
        "        # nasty tf.while_loop and tf.cond ops\n",
        "        while_op = tf.while_loop(\n",
        "            (lambda i, _: tf.less(i, max_num+1)), \n",
        "            (lambda i, _: (tf.add(i,1), tf.cond(\n",
        "                tf.logical_and(tf.equal(tf.mod(i, 3), 0), tf.equal(tf.mod(i, 5), 0)),\n",
        "                (lambda : tf.assign(arr[(i - 1)], 'FizzBuzz')),\n",
        "                (lambda : tf.cond(tf.equal(tf.mod(i, 3), 0),\n",
        "                    (lambda : tf.assign(arr[(i - 1)], 'Fizz')),\n",
        "                    (lambda : tf.cond(tf.equal(tf.mod(i, 5), 0),\n",
        "                        (lambda : tf.assign(arr[(i - 1)], 'Buzz')),\n",
        "                        (lambda : arr)))))))),\n",
        "            [1, arr])\n",
        "\n",
        "    # Call Session.run()\n",
        "    with tf.Session(graph = graph) as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        idx, array = sess.run(while_op)\n",
        "        print(array)\n",
        "\n",
        "\n",
        "fizzbuzz_graph(100)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[b'1' b'2' b'Fizz' b'4' b'Buzz' b'Fizz' b'7' b'8' b'Fizz' b'Buzz' b'11'\n",
            " b'Fizz' b'13' b'14' b'FizzBuzz' b'16' b'17' b'Fizz' b'19' b'Buzz' b'Fizz'\n",
            " b'22' b'23' b'Fizz' b'Buzz' b'26' b'Fizz' b'28' b'29' b'FizzBuzz' b'31'\n",
            " b'32' b'Fizz' b'34' b'Buzz' b'Fizz' b'37' b'38' b'Fizz' b'Buzz' b'41'\n",
            " b'Fizz' b'43' b'44' b'FizzBuzz' b'46' b'47' b'Fizz' b'49' b'Buzz' b'Fizz'\n",
            " b'52' b'53' b'Fizz' b'Buzz' b'56' b'Fizz' b'58' b'59' b'FizzBuzz' b'61'\n",
            " b'62' b'Fizz' b'64' b'Buzz' b'Fizz' b'67' b'68' b'Fizz' b'Buzz' b'71'\n",
            " b'Fizz' b'73' b'74' b'FizzBuzz' b'76' b'77' b'Fizz' b'79' b'Buzz' b'Fizz'\n",
            " b'82' b'83' b'Fizz' b'Buzz' b'86' b'Fizz' b'88' b'89' b'FizzBuzz' b'91'\n",
            " b'92' b'93' b'94' b'95' b'96' b'97' b'98' b'99' b'Buzz']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10w0TYJN8p5L"
      },
      "source": [
        "TensorFlow2 can still benefit from graph-based execution as it provides `tf.function` where we can define a function with operations. TensorFlow constructs a graph for the function automatically and apply possible optimizations.\n",
        "\n",
        "For more information on `tf.function`, refer to this website: https://www.tensorflow.org/guide/function?hl=ko"
      ]
    }
  ]
}