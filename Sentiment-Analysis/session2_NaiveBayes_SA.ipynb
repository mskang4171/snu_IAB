{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python: IAB",
      "language": "python",
      "name": "conda-env-IAB-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "2020_2학기_assignment_session2_NaiveBayes_SA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "xJAdWpSalvIR"
      },
      "source": [
        "# Session #2\n",
        "## ML-based SA by using Naive Bayes\n",
        "\n",
        "두 번째 세션에서는 Naive Bayes Calssifier 를 이용한 영어 텍스트 감성분석 프로그램을 작성해 봅니다. \n",
        "실습 수업은 프로그램의 주요 흐름을 설명하면서, TODO 처리된 핵심적인 부분의 코드를 직접 작성해보는 순서로 진행될 것입니다.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "IU1UPqmRlvIT"
      },
      "source": [
        "from __future__ import print_function # python 2 and 3 compatibility\n",
        "import io\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from os import listdir\n",
        "import math\n",
        "import six\n",
        "\n",
        "#-*- coding: utf-8 -*-"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EITLrk1clvIg",
        "outputId": "bbf708f8-8121-46d4-f063-2f66af571864",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# download nltk package for word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbZzPhZkpGuu"
      },
      "source": [
        "### **구글 드라이브를 이용하여 데이터 import 및 압축풀기**\n",
        "a) 내 구글 드라이브 mount 하기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khkqrcgTpKF2",
        "outputId": "5e589b66-20b2-4467-e1a8-2a6747c226bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\"\"\" \n",
        "내 구글 드라이브와 colab 연결 ==> \n",
        "아래 셀 실행 후 출력되는 링크 클릭 ==> \n",
        "authorization code 복사 후 아래 출력된 박스에 붙여넣고 enter 키 입력\n",
        "\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3U82r7opLjm",
        "outputId": "dd97bdae-a3d7-43e9-b6dd-742b8e297354",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\"\"\"\n",
        "내 구글 드라이브와 잘 연동 되었는지 확인해보기\n",
        "실습 파일들을 내 구글 드라이브 (My Drive) 내에 다른 폴더를 만들었다면 ==> \n",
        "os.listdir('/content/drive/My Drive/내가_만든_폴더_이름')\n",
        "\"\"\"\n",
        "import os\n",
        "os.listdir('/content/drive/My Drive/Colab Notebooks/20.10.30')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['6humanCodedDataSets.zip',\n",
              " 'SentiStrength_Data.zip',\n",
              " 'data.zip',\n",
              " 'session1_dictionary_SA.ipynb',\n",
              " '2020_2학기_assignment_session1_dictionary_SA.ipynb',\n",
              " 'session2_NaiveBayes_SA.ipynb',\n",
              " '2020_2학기_assignment_session2_NaiveBayes_SA.ipynb']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjD5KzgLpMcu"
      },
      "source": [
        "\"\"\"\n",
        "구글 드라이브에 올려놓은 실습 데이터 zip 압축 풀기 ==> \n",
        "zipfile 의 extractall 함수 이용 \n",
        "압축을 풀 경로 (directory_to_extract_to)는 반드시 '/tmp/' 아래 지정할 것 ex) '/tmp/data/'\n",
        "\"\"\"\n",
        "import zipfile\n",
        "path_to_zip_file = '/content/drive/My Drive/Colab Notebooks/20.10.30/data.zip'\n",
        "directory_to_extract_to = '/tmp/data'\n",
        "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
        "  zip_ref.extractall(directory_to_extract_to)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "tOt9IFrblvIs"
      },
      "source": [
        "## variable 선언 부\n",
        "- 확률 계산 값을 저장할 변수 및 file path 를 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "A-Qmk2mBlvIv"
      },
      "source": [
        "# Variables\n",
        "\n",
        "voca_dic = {}             # token dictionary\n",
        "log_prior_pos = 1         # prior probability for positive class\n",
        "log_prior_neg = 1         # prior probability for negative class\n",
        "\n",
        "log_likelihood_pos = {}   \n",
        "log_likelihood_neg = {}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "WCfHn0XWlvI0"
      },
      "source": [
        "## step 1. Compute the prior probability\n",
        "- 각 class 의 (positive, negative) prior 확률을 구해봅니다.\n",
        "- training set 상에서 positive / negative class 에 속하는 data 의 개수를 count 하는 방법으로 구할 수 있습니다.\n",
        "- 본 실험에서는 file 의 개수를 count 하는 것으로 구해봅니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "CbvULV8hlvI1"
      },
      "source": [
        "'''\n",
        "count # of files in class path\n",
        "Input: data path\n",
        "Output: # of data\n",
        "'''\n",
        "def count_file(dir_path):\n",
        "    \n",
        "    '''\n",
        "    -- TODO -- \n",
        "    list files = ... \n",
        "    '''\n",
        "\n",
        "    list_files = [f for f in listdir(dir_path)]\n",
        "    \n",
        "    return len(list_files)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "_jVjm481lvI6"
      },
      "source": [
        "사전 데이터와 테스트에 사용할 텍스트 파일이 있는 폴더를 지정합니다.  \n",
        "\n",
        "마운트된 구글 드라이브 사용하여 압축 해제한 폴더 지정 (/tmp/data/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "BBTT84aHlvI7",
        "outputId": "76bfef34-4ef7-47af-e686-51755a6be548",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data_root_path = '/tmp/data'\n",
        "\n",
        "#target_data = data_root_path + '/sample'\n",
        "target_data = data_root_path + '/train'\n",
        "\n",
        "log_prior_pos = math.log(count_file(target_data + '/pos'))\n",
        "log_prior_neg = math.log(count_file(target_data + '/neg'))\n",
        "\n",
        "print('* log prior of the positive class: ' + str(log_prior_pos))\n",
        "print('* log_prior of the negative class: ' + str(log_prior_neg ))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "* log prior of the positive class: 9.433483923290392\n",
            "* log_prior of the negative class: 9.433483923290392\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "PP8PoV4_lvI_"
      },
      "source": [
        "## step 2. Compute the Likelihood\n",
        "- 주어진 문장과 각 class 간 likelihood 값을 계산해 봅니다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "DCtmF3UTlvJA"
      },
      "source": [
        "### step 2-1. Create Vocabulary Dictionary\n",
        "- 현재 data set 에서 사용되는 모든 token 을 파악하기 위해서 전체 data set 에 들어 있는 token 을 포함하는 dictionary 를 생성합니다.\n",
        "- 특정 폴더 안에 있는 포든 파일을 읽어서 해당 파일에 들어 있는 문장을 tokenize 후 얻어진 token 을 dictioinary 에 추가 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "19W9RsdjlvJB"
      },
      "source": [
        "'''\n",
        "build token dictionary\n",
        "Input: data path, 최종 dictionary\n",
        "Return: None (dictionary 는 내부에서 update 됨)\n",
        "'''\n",
        "def build_dic(dir_path, dic):\n",
        "    \n",
        "    list_files = [f for f in listdir(dir_path)]\n",
        "\n",
        "    for file in list_files:\n",
        "      \n",
        "        try:\n",
        "            f = open(dir_path + file, 'r')\n",
        "            \n",
        "            '''\n",
        "            -- TODO -- \n",
        "            dir_path 내에 있는 파일들을 loop\n",
        "      \n",
        "            하나의 파일에 있는 내용을 읽은 후 tokenize\n",
        "            ex) tokens = word_tokenize(line.strip().lower())\n",
        "\n",
        "            tokens 에 담겨 있는 token 들을 dictionary 에 추가   \n",
        "            '''\n",
        "\n",
        "            line = f.readline()\n",
        "            tokens = word_tokenize(line.strip().lower())\n",
        "\n",
        "            for token in tokens:\n",
        "              if token not in dic:\n",
        "                dic[token] = 1\n",
        "\n",
        "            f.close()        \n",
        "        \n",
        "        except:\n",
        "            pass"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "02DOlPw0lvJG"
      },
      "source": [
        "positive, negative class 안에 들어 있는 모든 data 에서 token 을 추출하여 dictionary 를 완성 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "BZrlw3uUlvJH",
        "outputId": "be958cd9-859c-4146-eaec-71ac351ba96d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "build_dic(target_data + '/pos/', voca_dic)\n",
        "build_dic(target_data + '/neg/', voca_dic)\n",
        "print(\"* total voca size: \" + str(len(voca_dic)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "* total voca size: 114526\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "zKHbs5hQlvJL"
      },
      "source": [
        "### Step 2-2. 각 Class 별 token 의 확률 table 생성\n",
        "- 특정 class 내 data 를 모두 tokenize 한 후 각 token 의 수를 count 하여 해당 token 이 해당 class 에서 나타날 확률을 계산합니다.\n",
        "- 확률 값 계산시 제외 되는 token 이 없게 하기 위해 전체 dictionary 를 기본으로 가진 후 추가 계산을 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "ceLJjwnqlvJL"
      },
      "source": [
        "'''\n",
        "class 별 확률 table 생성\n",
        "Input: data path, 전체 dictionary\n",
        "Return: 해당 class 의 확률 테이블\n",
        "'''\n",
        "def create_class_dic(dir_path, base_dic):\n",
        "    \n",
        "    # copy base_dic and create likelihood_table\n",
        "    likelihood_table = {}\n",
        "    likelihood_table = dict( (nkey, 1) for nkey in [key for key in base_dic.keys()])\n",
        "        \n",
        "    list_files = [f for f in listdir(dir_path)]\n",
        "\n",
        "    for file in list_files:\n",
        "      \n",
        "        try:\n",
        "            f = open(dir_path + file, 'r')\n",
        "            line = f.readline()  \n",
        "            tokens = word_tokenize(line.strip().lower())\n",
        "\n",
        "            for token in tokens:\n",
        "                \n",
        "                '''\n",
        "                -- TODO -- \n",
        "                likelihood table 을 update\n",
        "                '''\n",
        "                likelihood_table[token] = likelihood_table[token] + 1\n",
        "\n",
        "            f.close()     \n",
        "            \n",
        "        except:\n",
        "            pass\n",
        "            \n",
        "    return likelihood_table"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "uXhQ8TDOlvJP",
        "outputId": "8d20a6f3-81ca-4cb6-e4f3-28e78b6cea54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pos_table = create_class_dic(target_data + '/pos/', voca_dic)  \n",
        "neg_table = create_class_dic(target_data + '/neg/', voca_dic)  \n",
        "\n",
        "token = 'good'\n",
        "print('token = ' + token)\n",
        "print('# of tokens in positive class: \\t' + str(pos_table[token]))\n",
        "print('# of tokens in negative class: \\t' + str(neg_table[token]))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "token = good\n",
            "# of tokens in positive class: \t7445\n",
            "# of tokens in negative class: \t7196\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Iu_cbbSwlvJS"
      },
      "source": [
        "### Step 2-3. 확률 table 을 log 확률 값으로 변환\n",
        "- log 함수는 monotonically increasing 함수이므로 log 를 취한 값으로 확률을 계산해도 동일한 비교가 가능합니다.\n",
        "- 확률값 계산시 * (곱셈) 이 아닌 + (덧셈) 으로 계산 가능하기 때문에 연산이 수월 합니다.\n",
        "- 곱셈으로 확률을 계산시 확률값이 매우 작아 질 경우 발생하는 수치에러를 방지할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "FcFCs-DDlvJT"
      },
      "source": [
        "'''\n",
        "log likelihood table 연산\n",
        "Input: 특정 class 의 확률 테이블\n",
        "Return: # of data\n",
        "'''\n",
        "def compute_log_likelihood_table(class_table):\n",
        "    \n",
        "    new_table = {}\n",
        "    word_sum = sum(class_table.values())\n",
        "    new_table = dict( (key, math.log((float)(value)/(float)(word_sum)) ) for (key, value) in six.iteritems(class_table)) \n",
        "    \n",
        "    return new_table"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Eqe3a4JBlvJV",
        "outputId": "58e9b658-9a01-45d2-c261-7342fa782b7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "log_likelihood_pos = compute_log_likelihood_table(pos_table)\n",
        "log_likelihood_neg = compute_log_likelihood_table(neg_table)\n",
        "\n",
        "token = 'good'\n",
        "print('token = ' + token)\n",
        "print('# of token in class: \\t\\t\\t' + str(pos_table[token]))\n",
        "print('probability of the token in class: \\t' + str(pos_table[token] / float(sum(pos_table.values()))))\n",
        "print('log probability: \\t\\t\\t' + str(log_likelihood_pos[token]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "token = good\n",
            "# of token in class: \t\t\t7445\n",
            "probability of the token in class: \t0.0020286954820447505\n",
            "log probability: \t\t\t-6.20036231217936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "iIF88sdilvJY"
      },
      "source": [
        "## Step 3. 하나의 문서를 분류해 보세요\n",
        "\n",
        "- 클래스별 log_table 을 이용해서 classifier 를 구현해 보세요\n",
        "- positive class 에 속할 확률 vs negative class 에 속할 확률"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "xyPgMovilvJY"
      },
      "source": [
        "'''\n",
        "특정 document 에 들어 있는 문장을 분류 (positive/negative)\n",
        "Input: document, positive class prior 확률, negative class prior 확률, positive class likelihood 테이블, negative class likelihood 테이블\n",
        "Return: 클래스\n",
        "'''\n",
        "def classify_doc(document, log_prior_pos, log_prior_neg, log_likelihood_pos, log_likelihood_neg):\n",
        "    \n",
        "    pos_prob = 0\n",
        "    neg_prob = 0\n",
        "    \n",
        "    tokens = word_tokenize(document.strip().lower())\n",
        "\n",
        "    for token in tokens:\n",
        "\n",
        "        '''\n",
        "        -- TODO -- \n",
        "        pos_prob 에 주어진 토큰의 해당 클래스에 따른 확률 값을 누적\n",
        "        neg_prob 에 주어진 토큰의 해당 클래스에 따른 확률 값을 누적\n",
        "        '''\n",
        "        if token in log_likelihood_pos:\n",
        "          pos_prob += log_likelihood_pos[token]\n",
        "        if token in log_likelihood_neg:\n",
        "          neg_prob += log_likelihood_neg[token]\n",
        "\n",
        "    pos_prob = pos_prob + log_prior_pos       \n",
        "    neg_prob = neg_prob + log_prior_neg\n",
        "\n",
        "    if pos_prob > neg_prob:\n",
        "        return 'positive' \n",
        "    else:\n",
        "        return 'negative'"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "2-vDzTnclvJb",
        "outputId": "1ed16356-d7d5-4c32-895b-4796283d7cc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "document = 'this is  my favorite. so exciting'\n",
        "# document = 'i hate the move.. jsut waste of the time'\n",
        "\n",
        "ret = classify_doc(document, log_prior_pos, log_prior_neg, log_likelihood_pos, log_likelihood_neg)\n",
        "\n",
        "print('  input document  : \\t' + document)\n",
        "print('  predicted class   : \\t' + ret)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  input document  : \tthis is  my favorite. so exciting\n",
            "  predicted class   : \tpositive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "7gDy4IZWlvJe"
      },
      "source": [
        "## Step 4. 주어진 path 에 있는 모든 문서를 분류\n",
        "- 특정 폴더 안에 있는 모든 문서를 분류하고, 정확도를 측정해 본니다.\n",
        "- 주어진 폴더 안에는 같은 class 의 data 가 분류되어 들어 있습니다. \n",
        "- Train 시에 사용되지 않은 data 를 가지고 테스트를 해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "wIh6yJy4lvJe"
      },
      "source": [
        "'''\n",
        "특정 폴더 안에 있는 모든 data 분류\n",
        "Input: data path\n",
        "Return: None\n",
        "'''\n",
        "def evaluate_all(dir_path):\n",
        "\n",
        "    list_files = [f for f in listdir(dir_path)]\n",
        "\n",
        "    pos_cnt = 0\n",
        "    neg_cnt = 0\n",
        "    process_doc = 0\n",
        "\n",
        "    for file in list_files:\n",
        "        '''\n",
        "        -- TODO -- \n",
        "        파일을 읽은 후 위에서 작성한 classify_doc() 함수를 이용하여 class 를 분류\n",
        "        '''\n",
        "        try:\n",
        "            f = open(dir_path + file, 'r')\n",
        "            line = f.readline()\n",
        "            result = classify_doc(line, log_prior_pos, log_prior_neg, log_likelihood_pos, log_likelihood_neg)\n",
        "            if(result == 'positive'):\n",
        "              pos_cnt += 1\n",
        "            else:\n",
        "              neg_cnt += 1\n",
        "            process_doc += 1\n",
        "            # f.close()\n",
        "        except:\n",
        "            pass\n",
        "    print(f\"Classify Summary for {dir_path} : \")\n",
        "    print(f\"Total classified documents : {process_doc} documents\")\n",
        "    print(f\"positive counted documents: {pos_cnt} documents\")\n",
        "    print(f\"negative counted documents: {neg_cnt} documents\")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "y1Is30D7lvJh",
        "outputId": "ae652045-07fd-4b13-95fd-39b33cb0e304",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "evaluate_all(data_root_path + '/test/neg/')\n",
        "evaluate_all(data_root_path + '/test/pos/')    "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classify Summary for /tmp/data/test/neg/ : \n",
            "Total classified documents : 12500 documents\n",
            "positive counted documents: 1559 documents\n",
            "negative counted documents: 10941 documents\n",
            "Classify Summary for /tmp/data/test/pos/ : \n",
            "Total classified documents : 12500 documents\n",
            "positive counted documents: 9297 documents\n",
            "negative counted documents: 3203 documents\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}